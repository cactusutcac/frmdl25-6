{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/azendohsaurus/bdq-reproduction-no-encoder.52220b8b-f8c8-4f30-b890-000f6b67b992.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250603/auto/storage/goog4_request&X-Goog-Date=20250603T132314Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=879cfaf680f4ce26dc2e5cb9685694bf14dd18b41d65c9af3f269fcb5fd90544f6852fd12c1aeb625ee372e6a9626e1d0db05e77ccf5a98fba5b9504a7e78f4454851540e4bf6b8210fbc506c17607a3beaa2dba03a7edb899c02bf1961567a0e2c5bfced4284a121c1c33027e6bb1955919e2c907963c8640ac939a3fea6523ef344b36897455d81afaff68bad8f059564ee37edb3df6d54586d359fe9cd98f73f7cf87e4fedfc3dfa45e5298fa33a7de2181538f61151624ee0942d8288fe70174b30667d99ed12f3e2e49fa5b6cd248350bd7331e9e1d08c6b61d122838d6a14afe31c44d3653060921865e1001756958d81951379e274e49295f3b35b03b","timestamp":1748957015924}]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":12072812,"datasetId":7591779,"databundleVersionId":12598723}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Import IXMAS dataset","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Specify IXMAS dataset and labels folder","metadata":{}},{"cell_type":"code","source":"IXMAS_DATA_DIR = '/kaggle/input/ixmas-dataset/IXMAS/IXMAS'\nIXMAS_LABELS_DIR = '/kaggle/input/ixmas-dataset/ixmas_clips_6.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:27.094149Z","iopub.execute_input":"2025-06-05T17:18:27.094482Z","iopub.status.idle":"2025-06-05T17:18:27.102384Z","shell.execute_reply.started":"2025-06-05T17:18:27.094457Z","shell.execute_reply":"2025-06-05T17:18:27.101614Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 2. Load python dependencies","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Install required packages","metadata":{}},{"cell_type":"code","source":"!pip install \"git+https://github.com/facebookresearch/pytorchvideo.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:27.103558Z","iopub.execute_input":"2025-06-05T17:18:27.103804Z","iopub.status.idle":"2025-06-05T17:18:39.266565Z","shell.execute_reply.started":"2025-06-05T17:18:27.103788Z","shell.execute_reply":"2025-06-05T17:18:39.265915Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/pytorchvideo.git\n  Cloning https://github.com/facebookresearch/pytorchvideo.git to /tmp/pip-req-build-l7wrzfvn\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-req-build-l7wrzfvn\n  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 6cdc929315aab1b5674b6dcf73b16ec99147735f\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo==0.1.5)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo==0.1.5)\n  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo==0.1.5)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo==0.1.5)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo==0.1.5) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo==0.1.5)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (3.0.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (11.1.0)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo==0.1.5) (4.13.2)\nCollecting portalocker (from iopath->pytorchvideo==0.1.5)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo==0.1.5) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore->pytorchvideo==0.1.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore->pytorchvideo==0.1.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo==0.1.5) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo==0.1.5) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fvcore->pytorchvideo==0.1.5) (2024.2.0)\nDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=213014 sha256=b1dfa54f89a4dc4fc6da11da1f1f3834d43a9746f0951b488d16e14cf3ef2507\n  Stored in directory: /tmp/pip-ephem-wheel-cache-65frhjvy/wheels/e3/00/83/9078ad0db7245b3cb777e4afbc737c8a584a8fc347f4438962\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=0d1f7ad310c6edb934e4e33b0ea9b45a06c4f4b6c69f51545afd59b39b5eec1a\n  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=f989640703b417de8c9ad69a212bf45ab4edf2d3de3fa07c7255caf3fa854c68\n  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-14.4.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.1.1 pytorchvideo-0.1.5 yacs-0.1.8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 2.2 Import libraries","metadata":{}},{"cell_type":"code","source":"import cv2\nimport json\nimport os\nimport random\nimport re\nimport torch\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nfrom PIL import Image\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    ShortSideScale,\n    UniformTemporalSubsample\n)\nfrom torch import nn\nfrom torch.optim import SGD, Optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LRScheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms.functional import pil_to_tensor\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torchvision.transforms import Compose, Lambda, CenterCrop, Normalize, Resize\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:39.267494Z","iopub.execute_input":"2025-06-05T17:18:39.267750Z","iopub.status.idle":"2025-06-05T17:18:47.259121Z","shell.execute_reply.started":"2025-06-05T17:18:39.267724Z","shell.execute_reply":"2025-06-05T17:18:47.258339Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.261051Z","iopub.execute_input":"2025-06-05T17:18:47.261339Z","iopub.status.idle":"2025-06-05T17:18:47.349718Z","shell.execute_reply.started":"2025-06-05T17:18:47.261321Z","shell.execute_reply":"2025-06-05T17:18:47.349036Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"os.environ['COLAB_PATH'] = '/kaggle/working/checkpoints'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.350525Z","iopub.execute_input":"2025-06-05T17:18:47.351117Z","iopub.status.idle":"2025-06-05T17:18:47.354723Z","shell.execute_reply.started":"2025-06-05T17:18:47.351086Z","shell.execute_reply":"2025-06-05T17:18:47.353986Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 3. Define classes","metadata":{}},{"cell_type":"markdown","source":"### 3.1 IXMAS dataset and transformation classes","metadata":{}},{"cell_type":"code","source":"class IXMASBDQDataset(Dataset):\n    def __init__(self, root_dir, json_path, transform=None, split=None):\n        \"\"\"\n        Args:\n            root_dir (str): Folder containing .avi video files.\n            json_path (str): Path to ixmas_clips.json\n            transform (callable, optional): Optional transform to apply to [T, C, H, W] tensor.\n            split (str): 'train', 'val', or 'test'; filters dataset.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n\n        with open(IXMAS_LABELS_DIR, \"r\") as f:\n            all_clips = json.load(f)\n        \n        self.data = [c for c in all_clips if c[\"split\"] == split] if split else all_clips\n        self.action_label_map = {label: i for i, label in enumerate(sorted(set(c[\"label\"] for c in self.data)))}\n\n    def __len__(self):\n        return len(self.data)\n\n    def _load_clip(self, video_path):\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise FileNotFoundError(f\"Could not open video: {video_path}\")\n\n        frames = []\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            img = Image.fromarray(frame)\n            img_tensor = pil_to_tensor(img)\n            frames.append(img_tensor)\n\n        cap.release()\n        if len(frames) == 0:\n            raise RuntimeError(f\"No frames extracted from {video_path}\")\n\n        clip = torch.stack(frames)  # [T, C, H, W]\n        return self.transform(clip) if self.transform else clip\n\n    def __getitem__(self, idx):\n        entry = self.data[idx]\n        video_path = os.path.join(self.root_dir, entry[\"video_id\"])\n        clip = self._load_clip(video_path)\n        action_label = self.action_label_map[entry[\"label\"]]\n        subject_id = entry[\"subject\"]\n        return clip, action_label, subject_id\n\nclass ConsecutiveTemporalSubsample(object):\n    \"\"\"\n    Sequentially subsamples num_samples indices from middle of a video formatted\n    as a ``torch.Tensor`` of shape (T, C, H, W).\n    \"\"\"\n\n    def __init__(self, num_samples):\n        \"\"\"\n        Args:\n            num_samples (int): The number of sequential samples to be selected.\n        \"\"\"\n        assert isinstance(num_samples, (int))\n        self.num_samples = num_samples\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): video tensor with shape (T, C, H, W).\n        \"\"\"\n        t = x.shape[0]\n        if self.num_samples >= t:\n            return x\n\n        offset = (t-self.num_samples) // 2\n        return x[offset:(offset+self.num_samples), ...]\n\nclass MultiScaleCrop(object):\n    \"\"\"\n    Randomly chooses a spatial position and scale from a list of scales\n    to perform a crop on a video.\n    \"\"\"\n    def __init__(self, scales=[1., 1./(2.**(0.25)), 1./(2.**(0.75)), 1./2.]):\n        \"\"\"\n        Args:\n            scales (list): a list of possible scales for multi-scale cropping.\n        \"\"\"\n        self.scales = scales\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): video tensor of shape (T, C, H, W).\n        \"\"\"\n        h, w = x.shape[-2:]\n        base_size = min(h, w)\n        crop_sizes = [int(base_size * scale) for scale in self.scales]\n\n        # Based on the code from https://arxiv.org/abs/2208.02459: choose a random crop width and height\n        # from potential ones. The crop size scales can differ by at most 1 index.\n        pairs = []\n        for i, crop_h in enumerate(crop_sizes):\n            for j, crop_w in enumerate(crop_sizes):\n                if abs(i-j) <= 1:\n                    pairs.append((crop_w, crop_h))\n\n        crop_w, crop_h = random.choice(pairs)\n\n        # Randomly sample the positional offset\n        offset_w, offset_h = self._sample_offset(w, h, crop_w, crop_h)\n\n        # Return cropped video\n        return x[:, :, offset_h:offset_h+crop_h, offset_w:offset_w+crop_w]\n\n    def _sample_offset(self, w, h, crop_w, crop_h):\n        \"\"\"\n        Randomly samples the spatial position offset.\n\n        Args:\n            w (int): width of video frame.\n            h (int): height of video frame.\n            crop_w (int): width of cropped frame.\n            crop_h (int): height of cropped frame.\n        \"\"\"\n        w_step = (w - crop_w) // 4\n        h_step = (h - crop_h) // 4\n\n        options = [\n            (0, 0),  # top-left\n            (4 * w_step, 0),  # top-right\n            (0, 4 * h_step),  # bottom-left\n            (4 * w_step, 4 * h_step),  # bottom-right\n            (2 * w_step, 2 * h_step),  # center\n\n            (0, 2 * h_step),  # center-left\n            (4 * w_step, 2 * h_step),  # center-right\n            (2 * w_step, 4 * h_step),  # bottom-center\n            (2 * w_step, 0),  # top-center\n            (1 * w_step, 1 * h_step),  # upper-left quarter\n            (3 * w_step, 1 * h_step),  # upper-right quarter\n            (1 * w_step, 3 * h_step),  # lower-left quarter\n            (3 * w_step, 3 * h_step),  # lower-right quarter\n        ]\n\n        return random.choice(options)\n\nclass NormalizePixelValues(object):\n    \"\"\"\n    Normalizes pixel values to be in the range [0., 1.] instead of the hex format.\n    \"\"\"\n    def __init__(self, eps=1e-6):\n        \"\"\"\n        Args:\n            eps (float): small offset to prevent edge values.\n        \"\"\"\n        self.eps = eps\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): an image-like tensor whose values to normalize.\n        \"\"\"\n        return torch.clamp(x / 255., self.eps, 1.-self.eps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.355503Z","iopub.execute_input":"2025-06-05T17:18:47.355753Z","iopub.status.idle":"2025-06-05T17:18:47.375534Z","shell.execute_reply.started":"2025-06-05T17:18:47.355735Z","shell.execute_reply":"2025-06-05T17:18:47.374845Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 3.2 Loss functions","metadata":{}},{"cell_type":"code","source":"class ActionLoss(nn.Module):\n    \"\"\"\n    Args:\n        encoder: the BDQ encoder\n        target_predictor: 3D CNN N for predicting target action attribute\n        alpha: the adversarial weight for trade-off between action and privacy recognition\n    \"\"\"\n    def __init__(self, alpha=1):\n        super().__init__()\n        self.alpha = alpha\n        self.cross_entropy = nn.CrossEntropyLoss()\n\n    def entropy(self, x, dim=1, eps=1e-6):\n        x = torch.clamp(x, eps, 1)\n        return -torch.mean(torch.sum(x * torch.log(x), dim=dim))\n\n    \"\"\"\n    Args:\n        T_pred: predicted target labels for the input video\n        P_pred: predicted privacy labels for the input video\n        L_action: the ground-truct action labels of the inputs\n    \"\"\"\n    def forward(self, T_pred, P_pred, L_action):\n        loss = self.cross_entropy(T_pred, L_action) - self.alpha * self.entropy(P_pred)\n        return loss\n\nclass PrivacyLoss(nn.Module):\n    \"\"\"\n    Args:\n        privacy_predictor: 2D CNN for predicting the privacy attribute\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.cross_entropy = nn.CrossEntropyLoss()\n\n    \"\"\"\n    Args:\n        P_pred: predicted privacy labels for the input video\n        L_privacy: the ground-truth privacy labels\n        fixed_encoder: the (fixed) BDQ encoder\n    \"\"\"\n    def forward(self, P_pred, L_privacy):\n        loss = self.cross_entropy(P_pred, L_privacy)\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.376314Z","iopub.execute_input":"2025-06-05T17:18:47.376593Z","iopub.status.idle":"2025-06-05T17:18:47.393891Z","shell.execute_reply.started":"2025-06-05T17:18:47.376573Z","shell.execute_reply":"2025-06-05T17:18:47.393327Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 3.3 BDQ encoder modules","metadata":{}},{"cell_type":"code","source":"class LearnableGaussian(nn.Module):\n    def __init__(self, kernel_size=5, init_sigma=1.0):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.sigma = nn.Parameter(torch.tensor(init_sigma))\n\n    def forward(self, x):\n        # Make sure sigma is positive\n        sigma = self.sigma\n\n        # x: (B, T, C, H, W)\n        # merge B, T and C to apply/learn same kernel for all channels\n        B, T, C, H, W = x.shape\n        x = x.view(-1, 1, H, W)\n        C_kernel = 1 #TODO initially was =C\n\n        # Create 1D kernel\n        k = self.kernel_size // 2\n        coords = torch.arange(-k, k + 1, dtype=torch.float32, device=x.device)\n        gauss = torch.exp(-0.5 * (coords / sigma)**2)\n        gauss = gauss / gauss.sum()\n\n        # Make 2D kernel\n        kernel = 0.5 / (torch.pi * (sigma ** 2)) * torch.outer(gauss, gauss)\n        kernel = kernel.expand(C_kernel, 1, self.kernel_size, self.kernel_size)\n\n        # Apply depthwise convolution\n        x = F.conv2d(x, kernel, padding=k, groups=C_kernel)\n        return x.view(B, T, C, H, W)\n\nclass Difference(nn.Module):\n    def __init__(self):\n        super(Difference, self).__init__()\n        self.bvj = None\n\n    \"\"\"\n    Args:\n        x: the input frames tensor of shape (B, T, C, H, W), i.e. video with T frames\n    \"\"\"\n    def forward(self, x):\n        d = x.roll(-1, dims=1) - x\n        return d\n\nclass DifferentiableQuantization(nn.Module):\n    def __init__(self, num_bins=15, hardness=5.0, normalize_input=True, rescale_output=True):\n        \"\"\"\n        Args:\n            num_bins (int): Number of quantization bins N = 2^k (default 15).\n            hardness (float): Controls sigmoid sharpness; higher = closer to step function.\n            normalize_input (bool): Whether to normalize input to [0, num_bins] before quantizing.\n            rescale_output (bool): Whether to rescale output back to input's original value range.\n        \"\"\"\n        super().__init__()\n        self.num_bins = num_bins\n        self.hardness = hardness\n        self.normalize_input = normalize_input\n        self.rescale_output = rescale_output\n\n        # Initialize bin centers at [0.5, 1.5, ..., 14.5] for num_bins = 15\n        init_bins = torch.linspace(0.5, num_bins - 0.5, steps=num_bins)\n        self.bins = nn.Parameter(init_bins)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Input tensor of shape (B, T, C, H, W)\n        Returns:\n            Tensor: Quantized output of shape (B, T, C, H, W)\n        \"\"\"\n        orig_min, orig_max = x.min(), x.max() #TODO is it batch min/max?\n\n        if self.normalize_input:\n            qmin = 0.0\n            qmax = float(self.num_bins)\n            scale = (orig_max - orig_min) / (qmax - qmin)\n            scale = max(scale, 1e-4)\n            x = (x - orig_min) / (orig_max - orig_min + 1e-4) * (qmax - qmin)\n\n        # Expand for broadcasting\n        x_expanded = x.unsqueeze(-1)                        # Shape: [B, T, C, H, W, 1]\n        bin_centers = self.bins.view(1, 1, 1, 1, 1, -1).to(device)        # Shape: [1, 1, 1, 1, 1, num_bins]\n\n        # Sum of sigmoid activations\n        y = torch.sigmoid(self.hardness * (x_expanded - bin_centers)).sum(dim=-1)\n\n        if self.normalize_input and self.rescale_output:\n            y = y * scale + orig_min\n\n        return y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.394561Z","iopub.execute_input":"2025-06-05T17:18:47.394811Z","iopub.status.idle":"2025-06-05T17:18:47.406261Z","shell.execute_reply.started":"2025-06-05T17:18:47.394793Z","shell.execute_reply":"2025-06-05T17:18:47.405836Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 3.4 BDQ encoder and label predictors","metadata":{}},{"cell_type":"code","source":"class BDQEncoder(nn.Module):\n    \"\"\"\n    Sequentially combines the blur, difference and quantization parts\n    to form the BDQ encoder.\n    \"\"\"\n    def __init__(self, hardness=5.0):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            LearnableGaussian(),\n            Difference(),\n            DifferentiableQuantization(hardness=hardness),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: the input tensor (video frame).\n        \"\"\"\n        for layer in self.encoder:\n            x = layer.forward(x)\n\n        return x\n\n    def freeze(self):\n        \"\"\"\n        Freezes the parameters to prevent/pause learning.\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        \"\"\"\n        Resumes learning for BDQ encoder parameters.\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = True\n\nclass ActionRecognitionModel(nn.Module):\n    def __init__(self, fine_tune, num_classes = 400, id_to_classname = None):\n        super(ActionRecognitionModel, self).__init__()\n\n        # From action recognition file global variables\n        side_size = 256\n        mean = [0.45, 0.45, 0.45]\n        std = [0.225, 0.225, 0.225]\n        crop_size = 256\n        num_frames = 8\n        sampling_rate = 8\n        frames_per_second = 30\n        clip_duration = (num_frames * sampling_rate) / frames_per_second\n        self.start_sec = 0\n        self.end_sec = self.start_sec + clip_duration\n\n        model = torch.hub.load('facebookresearch/pytorchvideo', 'i3d_r50', pretrained=True)\n        model = model.eval()\n        model = model.to(device)\n        if fine_tune:\n            model.blocks[-1].proj = nn.Linear(in_features = model.blocks[-1].proj.in_features, out_features = num_classes)\n        self.model = model\n        self.transform = ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose([UniformTemporalSubsample(num_frames),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    ShortSideScale(size = side_size),\n                    CenterCrop((crop_size, crop_size))])\n        )\n        self.id_to_classname = id_to_classname\n\n    #accepts [B?, C=3, T=num_frames?, crop_size, crop_size]\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): input video (batched) of shape (B, T, C, H, W).\n\n        outputs:\n            y (string): predicted action label.\n        \"\"\"\n        # If not batched, make sample of 1 batch\n        if len(x.shape) == 4:\n            x = x.unsqueeze(dim=0)\n\n        # Transpose channel and temporal dimension\n        x = torch.transpose(x, -3, -4)\n        logits = self.model(x)  # Get prediction logits from 3d resnet. Shape: (B, num_classes)\n\n        # Apply softmax to get and return probabilities of each label\n        logits_softmax = F.softmax(logits, dim=1)\n\n        return logits_softmax\n\n    def test(self, video_path):\n        video = EncodedVideo.from_path(video_path)\n        video_data = video.get_clip(start_sec = self.start_sec, end_sec =self.end_sec)\n        video_data = self.transform(video_data)\n        inputs = video_data[\"video\"]\n        return self.predict(inputs)\n\n    def predict(self, inputs):\n        inputs = inputs.to(device)\n        preds = self.forward(inputs[None, ...])\n        post_act = torch.nn.Softmax(dim = 1)\n        preds = post_act(preds)\n        pred_classes = preds.topk(k = 1).indices[0]\n        pred_class_names = [self.id_to_classname[int(i)] for i in pred_classes]\n        return pred_class_names\n\n    def freeze(self):\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.parameters():\n            param.requires_grad = True\n# Adapted from: https://pytorch.org/hub/facebookresearch_pytorchvideo_resnet/\n\nclass PrivacyAttributePredictor(nn.Module):\n    \"\"\"\n    Privacy Attribute Prediction Model.\n    Uses a 2D ResNet-50 to predict privacy attributes from BDQ-encoded video frames.\n    The softmax outputs from each frame are averaged.\n    \"\"\"\n    def __init__(self, num_privacy_classes, pretrained_resnet=True):\n        \"\"\"\n        Args:\n            num_privacy_classes (int): The number of privacy attribute classes to predict.\n            pretrained_resnet (bool): Whether to use ImageNet pre-trained weights for ResNet-50.\n        \"\"\"\n        super().__init__()\n        self.num_privacy_classes = num_privacy_classes\n\n        # Load a 2D ResNet-50 model\n        if pretrained_resnet:\n            self.resnet_feature_extractor = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        else:\n            self.resnet_feature_extractor = models.resnet50(weights=None)\n\n        # Replace the final fully connected layer for the new number of privacy classes\n        num_ftrs = self.resnet_feature_extractor.fc.in_features\n        self.resnet_feature_extractor.fc = nn.Linear(num_ftrs, num_privacy_classes)\n\n    def forward(self, bdq_encoded_video):\n        \"\"\"\n        Forward pass for the privacy attribute predictor.\n\n        Args:\n            bdq_encoded_video (torch.Tensor): The output from the BDQ encoder.\n                Shape: (B, T, C, H, W), where\n                B = batch size\n                T = number of time steps/frames\n                C = number of channels\n                H = height\n                W = width\n\n        Returns:\n            torch.Tensor: Averaged softmax probabilities for privacy attributes.\n                          Shape: (B, num_privacy_classes)\n        \"\"\"\n        B, T, C, H, W = bdq_encoded_video.shape\n\n        # ResNet50 expects input of shape (N, C, H, W).\n        # We need to process each of the T frames for each video in the batch.\n        # Reshape to (B*T, C, H, W) to pass all frames through ResNet in one go.\n        video_reshaped_for_resnet = bdq_encoded_video.contiguous().view(B * T, C, H, W)\n\n        # Get logits from the ResNet feature extractor for all (B*T) frames\n        logits_all_frames = self.resnet_feature_extractor(video_reshaped_for_resnet) # Shape: (B*T, num_privacy_classes)\n\n        # Apply softmax to get probabilities for each frame\n        softmax_all_frames = F.softmax(logits_all_frames, dim=1) # Shape: (B*T, num_privacy_classes)\n\n        # Reshape back to (B, T, num_privacy_classes) to separate frames per video\n        softmax_per_frame_per_video = softmax_all_frames.view(B, T, self.num_privacy_classes)\n\n        # Average the softmax outputs over the T frames for each video in the batch\n        # as described in the paper (Section 4.2 Validation & Section 4.3 Results explanation).\n        averaged_softmax_predictions = torch.mean(softmax_per_frame_per_video, dim=1) # Shape: (B, num_privacy_classes)\n\n        return averaged_softmax_predictions\n\n    def freeze(self):\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.parameters():\n            param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.406862Z","iopub.execute_input":"2025-06-05T17:18:47.407073Z","iopub.status.idle":"2025-06-05T17:18:47.425308Z","shell.execute_reply.started":"2025-06-05T17:18:47.407052Z","shell.execute_reply":"2025-06-05T17:18:47.424673Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 4. Define model training function","metadata":{}},{"cell_type":"code","source":"# Setup checkpointing\nCOLAB_PATH = os.getenv('COLAB_PATH')\nCHECKPOINT_PATH = \"checkpoints\" if COLAB_PATH is None else COLAB_PATH  # \"checkpoints/checkpoint_1.tar\"\nif not os.path.isdir(CHECKPOINT_PATH):\n    os.makedirs(CHECKPOINT_PATH)\n\n\ndef get_sorted_checkpoints():\n    checkpoints = []\n    try:\n        files = os.listdir(CHECKPOINT_PATH)\n    except FileNotFoundError:\n        return checkpoints\n    for file in files:\n        match = re.search(r'checkpoint_(\\d+)\\.tar$', file)\n        if match:\n            checkpoints.append((os.path.join(CHECKPOINT_PATH, file), int(match.group(1))))\n    checkpoints.sort(key=lambda x: x[1])\n    return checkpoints\n\ndef delete_old_checkpoints():\n    checkpoints = get_sorted_checkpoints()\n    if len(checkpoints) > 2:\n        for file, _ in checkpoints[:-2]:\n            os.remove(file)\n\ndef compute_accuracy(input, target_action, target_privacy):\n    \"\"\"\n    Computes action and privacy prediction accuracy\n    Args:\n        input: the input (batched) video tensor\n        target_action: target labels for action attribute\n        target_privacy: target labels for privacy attribute\n    \"\"\"\n    with torch.no_grad():\n        input_encoded = E.forward(input)\n        T_pred = T.forward(input_encoded).argmax(dim=1)\n        P_pred = P.forward(input_encoded).argmax(dim=1)\n\n        action_acc = torch.sum(T_pred == target_action)\n        privacy_acc = torch.sum(P_pred == target_privacy)\n\n        return action_acc, privacy_acc\n\ndef train_once(train_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n               action_loss: ActionLoss, privacy_loss: PrivacyLoss, optimizer_ET: Optimizer, optimizer_P: Optimizer):\n    \"\"\"\n    Function to perform one training epoch of adverserial training from https://arxiv.org/abs/2208.02459\n    Args:\n        train_dataloader: DataLoader for the training split of the IXMAS dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n        optimizer_ET: SGD optimizer for the encoder and action attribute predictor\n        optimizer_P: SGD optimizer for the privacy attribute predictor\n    \"\"\"\n    # Set all components to training mode\n    E.train()\n    T.train()\n    P.train()\n\n    total_loss_action = torch.tensor(0.)\n    total_loss_privacy = torch.tensor(0.)\n    total_acc_action = torch.tensor(0.)\n    total_acc_privacy = torch.tensor(0.)\n\n    for input, target_action, target_privacy in tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training epoch...\", unit=\"batch\", position=1, leave=False):\n        input = input.to(device)\n        target_action = target_action.to(device)\n        target_privacy = target_privacy.to(device)\n\n        # Reset gradients\n        optimizer_P.zero_grad()\n        optimizer_ET.zero_grad()\n\n        # Freeze P, train E and T together\n        P.freeze()\n        input_encoded = E.forward(input)\n        action_pred = T.forward(input_encoded)\n        frozen_privacy_pred = P.forward(input_encoded)\n        loss_action = action_loss.forward(action_pred, frozen_privacy_pred, target_action)\n        loss_action.backward()\n        optimizer_ET.step()\n\n        # Freeze E and T, unfreeze and train P\n        P.unfreeze()\n        E.freeze()\n        T.freeze()\n        frozen_input_encoded = E.forward(input)\n        privacy_pred = P.forward(frozen_input_encoded)\n        loss_privacy = privacy_loss.forward(privacy_pred, target_privacy)\n        loss_privacy.backward()\n        optimizer_P.step()\n\n        # Unfreeze all models, record losses\n        E.unfreeze()\n        T.unfreeze()\n\n        # Compute statistics\n        acc_action, acc_privacy = compute_accuracy(input, target_action, target_privacy)\n\n        total_loss_action += loss_action.item()\n        total_loss_privacy += loss_privacy.item()\n\n        total_acc_action += acc_action.item()\n        total_acc_privacy += acc_privacy.item()\n\n    # Average out accuracies\n    total_acc_action /= len(train_dataloader.dataset)\n    total_acc_privacy /= len(train_dataloader.dataset)\n\n    return total_loss_action, total_loss_privacy, total_acc_action, total_acc_privacy\n\ndef validate_once(val_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n                  action_loss: ActionLoss, privacy_loss: PrivacyLoss):\n    \"\"\"\n    Function to perform one validation epoch of adverserial training from https://arxiv.org/abs/2208.02459\n    Args:\n        val_dataloader: DataLoader for the validation split of the IXMAS dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n    \"\"\"\n    E.eval()\n    T.eval()\n    P.eval()\n\n    with torch.no_grad():\n\n        total_loss_action = torch.tensor(0.)\n        total_loss_privacy = torch.tensor(0.)\n        total_acc_action = torch.tensor(0.)\n        total_acc_privacy = torch.tensor(0.)\n\n        for input, target_action, target_privacy in tqdm(val_dataloader, total=len(val_dataloader), desc=\"Validating epoch...\", unit=\"batch\", position=1, leave=False):\n            input = input.to(device)\n            target_action = target_action.to(device)\n            target_privacy = target_privacy.to(device)\n\n            # Perform evaluation with models on respective inputs\n            input_encoded = E.forward(input)\n            action_pred = T.forward(input_encoded)\n            privacy_pred = P.forward(input_encoded)\n\n            # Compute statistics\n            loss_action = action_loss.forward(action_pred, privacy_pred, target_action)\n            loss_privacy = privacy_loss.forward(privacy_pred, target_privacy)\n\n            acc_action, acc_privacy = compute_accuracy(input, target_action, target_privacy)\n\n            total_loss_action += loss_action.item()\n            total_loss_privacy += loss_privacy.item()\n            total_acc_action += acc_action.item()\n            total_acc_privacy += acc_privacy.item()\n\n        # Average out accuracies\n        total_acc_action /= len(val_dataloader.dataset)\n        total_acc_privacy /= len(val_dataloader.dataset)\n\n        return total_loss_action, total_loss_privacy, total_acc_action, total_acc_privacy\n\ndef adverserial_training(train_dataloader: DataLoader, val_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel,\n                         P: PrivacyAttributePredictor, optimizer_ET: Optimizer, optimizer_P: Optimizer, scheduler_ET: LRScheduler,\n                         scheduler_P: LRScheduler, action_loss: ActionLoss, privacy_loss: PrivacyLoss, last_epoch=0, num_epochs=50):\n    \"\"\"\n    Function encapsulating the whole adverserial training process from https://arxiv.org/abs/2208.02459\n    Args:\n        train_dataloader: DataLoader for the training split of the IXMAS dataset\n        val_dataloader: DataLoader for the validation split of the IXMAS dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        optimizer_ET: SGD optimizer for the encoder and action attribute predictor\n        optimizer_P: SGD optimizer for the privacy attribute predictor\n        scheduler_ET: learning rate scheduler for updating learning rate each epoch for optimizer_ET\n        scheduler_P: learning rate scheduler for updating learning rate each epoch for optimizer_P\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n        last_epoch (optional, int): checkpoint of last saved epoch\n        num_epochs (optional, int): number of epochs to train for (default=50)\n    \"\"\"\n    def save_checkpoint(epoch: int):\n        torch.save({\n            'E_state_dict': E.state_dict(),\n            'T_state_dict': T.state_dict(),\n            'P_state_dict': P.state_dict(),\n            'optim_ET_state_dict': optimizer_ET.state_dict(),\n            'optim_P_state_dict': optimizer_P.state_dict(),\n            'scheduler_ET_state_dict': scheduler_ET.state_dict(),\n            'scheduler_P_state_dict': scheduler_P.state_dict(),\n        }, os.path.join(CHECKPOINT_PATH, f\"checkpoint_{epoch}.tar\"))\n        delete_old_checkpoints()\n\n    with tqdm(range(last_epoch, num_epochs), total=num_epochs, initial=last_epoch, desc=\"Averserial training\", unit=\"epoch\", position=0, leave=True) as progress_loader:\n        for epoch in progress_loader:\n            train_loss_action, train_loss_privacy, train_acc_action, train_acc_privacy = train_once(train_dataloader=train_dataloader, E=E, T=T, P=P,\n                                                                                                    action_loss=action_loss, privacy_loss=privacy_loss,\n                                                                                                    optimizer_ET=optimizer_ET, optimizer_P=optimizer_P)\n\n            val_loss_action, val_loss_privacy, val_acc_action, val_acc_privacy = validate_once(val_dataloader=val_dataloader, E=E, T=T, P=P,\n                                                                                            action_loss=action_loss, privacy_loss=privacy_loss)\n\n            # Update learning rates\n            scheduler_ET.step()\n            scheduler_P.step()\n            save_checkpoint(epoch + 1)\n\n            # Display statistics\n            progress_loader.set_postfix(action_loss=val_loss_action.numpy(), privacy_loss=val_loss_privacy.numpy(),\n                                         action_accuracy=val_acc_action.numpy(), privacy_accuracy= val_acc_privacy.numpy())\n            progress_loader.refresh()\n            # print(f\"Epoch {epoch+1}/{num_epochs}, Action Loss: {val_loss_action:.4f}, Privacy Loss: {val_loss_privacy:.4f}\")\n            # print(f\"Action accuracy: {val_acc_action:.4f}, Privacy accuracy: {val_acc_privacy:.4f}\")\n\ndef load_train_checkpoint(E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n               optim_ET: Optimizer, optim_P: Optimizer, scheduler_ET: LRScheduler, scheduler_P: LRScheduler, PATH: str | None):\n    if PATH is None:\n        return\n    checkpoint = torch.load(PATH, weights_only=True)\n    E.load_state_dict(checkpoint['E_state_dict'])\n    E.to(device)\n    T.load_state_dict(checkpoint['T_state_dict'])\n    T.to(device)\n    P.load_state_dict(checkpoint['P_state_dict'])\n    P.to(device)\n    optim_ET.load_state_dict(checkpoint['optim_ET_state_dict'])\n    optim_P.load_state_dict(checkpoint['optim_P_state_dict'])\n    scheduler_ET.load_state_dict(checkpoint['scheduler_ET_state_dict'])\n    scheduler_P.load_state_dict(checkpoint['scheduler_P_state_dict'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.426797Z","iopub.execute_input":"2025-06-05T17:18:47.426966Z","iopub.status.idle":"2025-06-05T17:18:47.448156Z","shell.execute_reply.started":"2025-06-05T17:18:47.426953Z","shell.execute_reply":"2025-06-05T17:18:47.447475Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 5. Train the models","metadata":{}},{"cell_type":"markdown","source":"This might help with memory fragmentation","metadata":{}},{"cell_type":"code","source":"# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.449077Z","iopub.execute_input":"2025-06-05T17:18:47.449287Z","iopub.status.idle":"2025-06-05T17:18:47.463869Z","shell.execute_reply.started":"2025-06-05T17:18:47.449271Z","shell.execute_reply":"2025-06-05T17:18:47.463333Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# print(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.464526Z","iopub.execute_input":"2025-06-05T17:18:47.464755Z","iopub.status.idle":"2025-06-05T17:18:47.478189Z","shell.execute_reply.started":"2025-06-05T17:18:47.464738Z","shell.execute_reply":"2025-06-05T17:18:47.477455Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Set parameters according to https://arxiv.org/abs/2208.02459\nnum_epochs = 50\nlr = 0.001\nbatch_size = 4\nconsecutive_frames = 24\ncrop_size = (224, 224)\n\n# Load IXMAS dataset. Apply transformation sequence according to Section 4.2 in https://arxiv.org/abs/2208.02459\ntrain_transform = Compose([\n    ConsecutiveTemporalSubsample(consecutive_frames), # first, sample 32 consecutive frames\n    MultiScaleCrop(), # then, apply randomized multi-scale crop\n    Resize(crop_size), # then, resize to (224, 224)\n    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n])\ntrain_data = IXMASBDQDataset(\n    root_dir=IXMAS_DATA_DIR,\n    json_path=IXMAS_LABELS_DIR,\n    transform=train_transform,\n    split=\"train\",\n)\ntrain_dataloader = DataLoader(\n    train_data,\n    batch_size=batch_size,\n    num_workers=8,            # ↑ more CPU workers\n    pin_memory=True,          # ↑ faster host→GPU copies\n    persistent_workers=True,  # ↑ reuse workers across epochs\n    prefetch_factor=2,        # ↑ keep 2 batches per worker ahead\n    shuffle=True,\n)\n# Load validation dataset according to the same Section 4.2\nval_transform = Compose([\n    ConsecutiveTemporalSubsample(consecutive_frames), # first sample 32 consecutive frames\n    CenterCrop(crop_size),  # then, we apply a center crop of (224, 224) without scaling (resizing)\n    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n])\nval_data = IXMASBDQDataset(\n    root_dir=IXMAS_DATA_DIR,\n    json_path=IXMAS_LABELS_DIR,\n    transform=val_transform,\n    split=\"val\",\n)\nval_dataloader = DataLoader(\n    val_data,\n    batch_size=batch_size,\n    num_workers=4,\n)\n\n# Initialize the BDQEncoder (E), the action attribute predictor (T),\n# and the privacy attribute predictor (P)\nE = BDQEncoder(hardness=5.0).to(device)\nT = ActionRecognitionModel(fine_tune=True, num_classes=12).to(device)\nP = PrivacyAttributePredictor(num_privacy_classes=10).to(device)\n\n# Initialize optimizer, scheduler and loss functions\noptim_ET = SGD(params=list(E.parameters())+list(T.parameters()), lr=lr)\noptim_P = SGD(params=list(P.parameters()), lr=lr)\nscheduler_ET = CosineAnnealingLR(optimizer=optim_ET, T_max=num_epochs)\nscheduler_P = CosineAnnealingLR(optimizer=optim_P, T_max=num_epochs)\ncheckpoints = get_sorted_checkpoints()\nlast_checkpoint_path = None\nlast_epoch = 0\nif len(checkpoints) > 0:\n    last_checkpoint_path, last_epoch = checkpoints[-1]\nload_train_checkpoint(E, T, P, optim_ET, optim_P, scheduler_ET, scheduler_P, last_checkpoint_path)\ncriterion_action = ActionLoss(alpha=1)\ncriterion_privacy = PrivacyLoss()\n\nadverserial_training(train_dataloader=train_dataloader, val_dataloader=val_dataloader, E=E, T=T, P=P,\n                      optimizer_ET=optim_ET, optimizer_P=optim_P, scheduler_ET=scheduler_ET,\n                      scheduler_P=scheduler_P, action_loss=criterion_action, privacy_loss=criterion_privacy,\n                      last_epoch=last_epoch, num_epochs=num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:18:47.478950Z","iopub.execute_input":"2025-06-05T17:18:47.479177Z","iopub.status.idle":"2025-06-05T17:58:30.810591Z","shell.execute_reply.started":"2025-06-05T17:18:47.479151Z","shell.execute_reply":"2025-06-05T17:58:30.809372Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\nDownloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/I3D_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/I3D_8x8_R50.pyth\n100%|██████████| 214M/214M [00:00<00:00, 264MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 204MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Averserial training:   0%|          | 0/50 [00:00<?, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e638c093974b6796a194354acb0b5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61ff6072f3e4799b721ce6ae7b2a1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training epoch...:   0%|          | 0/23 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating epoch...:   0%|          | 0/8 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":13}]}