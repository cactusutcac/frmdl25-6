{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b3248b",
   "metadata": {},
   "source": [
    "### Wrapper for training.py to test function behaviours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcfa92",
   "metadata": {},
   "source": [
    "#### Import training function and required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f724098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from BDQ import BDQEncoder\n",
    "from action_recognition_model import ActionRecognitionModel\n",
    "from loss import ActionLoss, PrivacyLoss\n",
    "from preprocess import KTHBDQDataset, ConsecutiveTemporalSubsample, MultiScaleCrop, NormalizePixelValues\n",
    "from privacy_attribute_prediction_model import PrivacyAttributePredictor\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from training import adverserial_training, load_train_checkpoint, get_sorted_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f5776",
   "metadata": {},
   "source": [
    "#### Perform adverserial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f90834",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['COLAB_PATH'] = \"../checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of KTH dataset and labels file\n",
    "KTH_DATA_DIR = \"../KTH\"\n",
    "KTH_LABELS_DIR = \"../kth_clips.json\"\n",
    "\n",
    "# Set parameters according to https://arxiv.org/abs/2208.02459\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "batch_size = 4\n",
    "consecutive_frames = 8\n",
    "crop_size = (224, 224)\n",
    "\n",
    "# Load KTH dataset. Apply transformation sequence according to Section 4.2 in https://arxiv.org/abs/2208.02459\n",
    "train_transform = Compose([\n",
    "    ConsecutiveTemporalSubsample(consecutive_frames), # first, sample 32 consecutive frames\n",
    "    MultiScaleCrop(), # then, apply randomized multi-scale crop\n",
    "    Resize(crop_size), # then, resize to (224, 224)\n",
    "    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n",
    "])\n",
    "train_data = KTHBDQDataset(\n",
    "    root_dir=KTH_DATA_DIR,\n",
    "    json_path=KTH_LABELS_DIR,\n",
    "    transform=train_transform,\n",
    "    split=\"train\",\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    ")\n",
    "# Load validation dataset according to the same Section 4.2\n",
    "val_transform = Compose([\n",
    "    ConsecutiveTemporalSubsample(consecutive_frames), # first sample 32 consecutive frames\n",
    "    CenterCrop(crop_size),  # then, we apply a center crop of (224, 224) without scaling (resizing)\n",
    "    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n",
    "])\n",
    "val_data = KTHBDQDataset(\n",
    "    root_dir=KTH_DATA_DIR,\n",
    "    json_path=KTH_LABELS_DIR,\n",
    "    transform=val_transform,\n",
    "    split=\"val\",\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Initialize the BDQEncoder (E), the action attribute predictor (T),\n",
    "# and the privacy attribute predictor (P)\n",
    "E = BDQEncoder(hardness=5.0).to(device)\n",
    "T = ActionRecognitionModel(fine_tune=True, num_classes=6).to(device)\n",
    "P = PrivacyAttributePredictor(num_privacy_classes=25).to(device)\n",
    "\n",
    "# Initialize optimizer, scheduler and loss functions\n",
    "optim_ET = SGD(params=list(E.parameters())+list(T.parameters()), lr=lr)\n",
    "optim_P = SGD(params=list(P.parameters()), lr=lr)\n",
    "scheduler_ET = CosineAnnealingLR(optimizer=optim_ET, T_max=num_epochs)\n",
    "scheduler_P = CosineAnnealingLR(optimizer=optim_P, T_max=num_epochs)\n",
    "checkpoints = get_sorted_checkpoints()\n",
    "last_checkpoint_path = None\n",
    "last_epoch = 0\n",
    "if len(checkpoints) > 0:\n",
    "    last_checkpoint_path, last_epoch = checkpoints[-1]\n",
    "load_train_checkpoint(E, T, P, optim_ET, optim_P, scheduler_ET, scheduler_P, last_checkpoint_path)\n",
    "criterion_action = ActionLoss(alpha=1)\n",
    "criterion_privacy = PrivacyLoss()\n",
    "\n",
    "adverserial_training(train_dataloader=train_dataloader, val_dataloader=val_dataloader, E=E, T=T, P=P, \n",
    "                        optimizer_ET=optim_ET, optimizer_P=optim_P, scheduler_ET=scheduler_ET, \n",
    "                        scheduler_P=scheduler_P, action_loss=criterion_action, privacy_loss=criterion_privacy,\n",
    "                        last_epoch=last_epoch, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
