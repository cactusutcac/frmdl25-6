{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/azendohsaurus/bdq-reproduction-no-encoder.52220b8b-f8c8-4f30-b890-000f6b67b992.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20250603/auto/storage/goog4_request&X-Goog-Date=20250603T132314Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=879cfaf680f4ce26dc2e5cb9685694bf14dd18b41d65c9af3f269fcb5fd90544f6852fd12c1aeb625ee372e6a9626e1d0db05e77ccf5a98fba5b9504a7e78f4454851540e4bf6b8210fbc506c17607a3beaa2dba03a7edb899c02bf1961567a0e2c5bfced4284a121c1c33027e6bb1955919e2c907963c8640ac939a3fea6523ef344b36897455d81afaff68bad8f059564ee37edb3df6d54586d359fe9cd98f73f7cf87e4fedfc3dfa45e5298fa33a7de2181538f61151624ee0942d8288fe70174b30667d99ed12f3e2e49fa5b6cd248350bd7331e9e1d08c6b61d122838d6a14afe31c44d3653060921865e1001756958d81951379e274e49295f3b35b03b","timestamp":1748957015924}],"gpuType":"T4","name":"BDQ_reproduction (no encoder)"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12058117,"sourceType":"datasetVersion","datasetId":7587966}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Import KTH dataset","metadata":{"id":"CNbnXICN4_YY"}},{"cell_type":"markdown","source":"### 1.1 Specify KTH dataset and labels folder","metadata":{"id":"8DMabUi0-IAK"}},{"cell_type":"code","source":"KTH_DATA_DIR = '/kaggle/input/kth-dataset-copy/KTH/KTH'\nKTH_LABELS_DIR = '/kaggle/input/kth-dataset-copy/kth_clips.json'","metadata":{"id":"Mf99nD6P-PBM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Load python dependencies","metadata":{"id":"F9VtoHkx5S_V"}},{"cell_type":"markdown","source":"### 2.1 Install required packages","metadata":{"id":"dehgU1xS8mVU"}},{"cell_type":"code","source":"!pip install \"git+https://github.com/facebookresearch/pytorchvideo.git\"","metadata":{"id":"BWgnzj368mHM","outputId":"7184a7bf-efa1-498a-fc6d-2da373cdaa2b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Import libraries","metadata":{"id":"EUgpEB7-8miy"}},{"cell_type":"code","source":"import re\nimport os\nimport cv2\nimport json\nimport random\n\nfrom PIL import Image\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD, Optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LRScheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision.transforms.functional import pil_to_tensor\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom torchvision.transforms import Compose, Lambda, CenterCrop, Normalize, Resize\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    ShortSideScale,\n    UniformTemporalSubsample\n)\n\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","metadata":{"id":"pQwPVReU5QZX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\"\nprint(device)","metadata":{"id":"JVe5tDCoDVO1","outputId":"98aa69ac-531d-48aa-e1a5-e8ff4c42cb59","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['COLAB_PATH'] = '/kaggle/working/checkpoints'","metadata":{"trusted":true,"id":"UfU_pO7K9fyC"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Define classes","metadata":{"id":"0jo5rWJv5xZd"}},{"cell_type":"markdown","source":"### 3.1 KTH dataset and transformation classes","metadata":{"id":"1h4VEBzl6bZ8"}},{"cell_type":"code","source":"ACTION_LABEL_MAP = {\n    \"boxing\": 0,\n    \"handclapping\": 1,\n    \"handwaving\": 2,\n    \"jogging\": 3,\n    \"running\": 4,\n    \"walking\": 5,\n}\n\nclass KTHBDQDataset(Dataset):\n    def __init__(self, root_dir, json_path, transform=None, split=None):\n        \"\"\"\n        Args:\n            root_dir (str): Path to KTH folder (contains 'boxing', 'handclapping', etc.)\n            json_path (str): JSON file with start/end frame info per clip\n            transform (callable, optional): Optional transform to be applied.\n            split (str, optional): Optional filter for 'train' / 'val' / 'test'\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n\n        with open(json_path, \"r\") as f:\n            all_clips = json.load(f)\n\n        self.data = all_clips if split is None else [clip for clip in all_clips if clip.get(\"split\") == split]\n\n    def __len__(self):\n        return len(self.data)\n\n    def _get_video_path(self, label, video_id):\n        \"\"\"Construct video path using the '_uncomp' suffix.\"\"\"\n        return os.path.join(self.root_dir, label, f\"{video_id}_uncomp.avi\")\n\n\n    def _load_clip(self, video_path, start, end):\n        \"\"\"Extract a clip of frames from the video\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        start = max(1, start)\n        end = min(end, total_frames)\n\n        indices = torch.arange(start, end+1).long().tolist()\n        frames = []\n\n        for i in range(total_frames):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_idx = i + 1\n            if frame_idx in indices:\n                img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                img_tensor = pil_to_tensor(img)\n                frames.append(img_tensor)\n\n        cap.release()\n\n        # Apply transformations at the end\n        if self.transform:\n            return self.transform(torch.stack(frames))\n        return torch.stack(frames)  # [T, C, H, W]\n\n    def __getitem__(self, idx):\n        entry = self.data[idx]\n        video_path = self._get_video_path(entry[\"label\"], entry[\"video_id\"])\n        clip = self._load_clip(video_path, entry[\"start_frame\"], entry[\"end_frame\"])\n        return clip, ACTION_LABEL_MAP[entry[\"label\"]], entry[\"subject\"]  # video tensor, action label, and privacy label\n\nclass ConsecutiveTemporalSubsample(object):\n    \"\"\"\n    Sequentially subsamples num_samples indices from middle of a video formatted\n    as a ``torch.Tensor`` of shape (T, C, H, W).\n    \"\"\"\n\n    def __init__(self, num_samples):\n        \"\"\"\n        Args:\n            num_samples (int): The number of sequential samples to be selected.\n        \"\"\"\n        assert isinstance(num_samples, (int))\n        self.num_samples = num_samples\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): video tensor with shape (T, C, H, W).\n        \"\"\"\n        t = x.shape[0]\n        if self.num_samples >= t:\n            return x\n\n        offset = (t-self.num_samples) // 2\n        return x[offset:(offset+self.num_samples), ...]\n\nclass MultiScaleCrop(object):\n    \"\"\"\n    Randomly chooses a spatial position and scale from a list of scales\n    to perform a crop on a video.\n    \"\"\"\n    def __init__(self, scales=[1., 1./(2.**(0.25)), 1./(2.**(0.75)), 1./2.]):\n        \"\"\"\n        Args:\n            scales (list): a list of possible scales for multi-scale cropping.\n        \"\"\"\n        self.scales = scales\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): video tensor of shape (T, C, H, W).\n        \"\"\"\n        h, w = x.shape[-2:]\n        base_size = min(h, w)\n        crop_sizes = [int(base_size * scale) for scale in self.scales]\n\n        # Based on the code from https://arxiv.org/abs/2208.02459: choose a random crop width and height\n        # from potential ones. The crop size scales can differ by at most 1 index.\n        pairs = []\n        for i, crop_h in enumerate(crop_sizes):\n            for j, crop_w in enumerate(crop_sizes):\n                if abs(i-j) <= 1:\n                    pairs.append((crop_w, crop_h))\n\n        crop_w, crop_h = random.choice(pairs)\n\n        # Randomly sample the positional offset\n        offset_w, offset_h = self._sample_offset(w, h, crop_w, crop_h)\n\n        # Return cropped video\n        return x[:, :, offset_h:offset_h+crop_h, offset_w:offset_w+crop_w]\n\n    def _sample_offset(self, w, h, crop_w, crop_h):\n        \"\"\"\n        Randomly samples the spatial position offset.\n\n        Args:\n            w (int): width of video frame.\n            h (int): height of video frame.\n            crop_w (int): width of cropped frame.\n            crop_h (int): height of cropped frame.\n        \"\"\"\n        w_step = (w - crop_w) // 4\n        h_step = (h - crop_h) // 4\n\n        options = [\n            (0, 0),  # top-left\n            (4 * w_step, 0),  # top-right\n            (0, 4 * h_step),  # bottom-left\n            (4 * w_step, 4 * h_step),  # bottom-right\n            (2 * w_step, 2 * h_step),  # center\n\n            (0, 2 * h_step),  # center-left\n            (4 * w_step, 2 * h_step),  # center-right\n            (2 * w_step, 4 * h_step),  # bottom-center\n            (2 * w_step, 0),  # top-center\n            (1 * w_step, 1 * h_step),  # upper-left quarter\n            (3 * w_step, 1 * h_step),  # upper-right quarter\n            (1 * w_step, 3 * h_step),  # lower-left quarter\n            (3 * w_step, 3 * h_step),  # lower-right quarter\n        ]\n\n        return random.choice(options)\n\nclass NormalizePixelValues(object):\n    \"\"\"\n    Normalizes pixel values to be in the range [0., 1.] instead of the hex format.\n    \"\"\"\n    def __init__(self, eps=1e-6):\n        \"\"\"\n        Args:\n            eps (float): small offset to prevent edge values.\n        \"\"\"\n        self.eps = eps\n\n    def __call__(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): an image-like tensor whose values to normalize.\n        \"\"\"\n        return torch.clamp(x / 255., self.eps, 1.-self.eps)","metadata":{"id":"ebv69K7R6bDv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Loss functions","metadata":{"id":"oHMhUBoc7fvW"}},{"cell_type":"code","source":"class ActionLoss(nn.Module):\n    \"\"\"\n    Args:\n        encoder: the BDQ encoder\n        target_predictor: 3D CNN N for predicting target action attribute\n        alpha: the adversarial weight for trade-off between action and privacy recognition\n    \"\"\"\n    def __init__(self, alpha=1):\n        super().__init__()\n        self.alpha = alpha\n        self.cross_entropy = nn.CrossEntropyLoss()\n\n    def entropy(self, x, dim=1, eps=1e-6):\n        x = torch.clamp(x, eps, 1)\n        return -torch.mean(torch.sum(x * torch.log(x), dim=dim))\n\n    \"\"\"\n    Args:\n        T_pred: predicted target labels for the input video\n        P_pred: predicted privacy labels for the input video\n        L_action: the ground-truct action labels of the inputs\n    \"\"\"\n    def forward(self, T_pred, P_pred, L_action):\n        loss = self.cross_entropy(T_pred, L_action) - self.alpha * self.entropy(P_pred)\n        return loss\n\nclass PrivacyLoss(nn.Module):\n    \"\"\"\n    Args:\n        privacy_predictor: 2D CNN for predicting the privacy attribute\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.cross_entropy = nn.CrossEntropyLoss()\n\n    \"\"\"\n    Args:\n        P_pred: predicted privacy labels for the input video\n        L_privacy: the ground-truth privacy labels\n        fixed_encoder: the (fixed) BDQ encoder\n    \"\"\"\n    def forward(self, P_pred, L_privacy):\n        loss = self.cross_entropy(P_pred, L_privacy)\n        return loss\n","metadata":{"id":"_qIkgC9K7gKu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 BDQ encoder modules","metadata":{"id":"ca9sv-CQ5XVx"}},{"cell_type":"code","source":"class LearnableGaussian(nn.Module):\n    def __init__(self, kernel_size=5, init_sigma=1.0):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.sigma = nn.Parameter(torch.tensor(init_sigma))\n\n    def forward(self, x):\n        # Make sure sigma is positive\n        sigma = self.sigma\n\n        # x: (B, T, C, H, W)\n        # merge B, T and C to apply/learn same kernel for all channels\n        B, T, C, H, W = x.shape\n        x = x.view(-1, 1, H, W)\n        C_kernel = 1 #TODO initially was =C\n\n        # Create 1D kernel\n        k = self.kernel_size // 2\n        coords = torch.arange(-k, k + 1, dtype=torch.float32, device=x.device)\n        gauss = torch.exp(-0.5 * (coords / sigma)**2)\n        gauss = gauss / gauss.sum()\n\n        # Make 2D kernel\n        kernel = 0.5 / (torch.pi * (sigma ** 2)) * torch.outer(gauss, gauss)\n        kernel = kernel.expand(C_kernel, 1, self.kernel_size, self.kernel_size)\n\n        # Apply depthwise convolution\n        x = F.conv2d(x, kernel, padding=k, groups=C_kernel)\n        return x.view(B, T, C, H, W)\n\nclass Difference(nn.Module):\n    def __init__(self):\n        super(Difference, self).__init__()\n        self.bvj = None\n\n    \"\"\"\n    Args:\n        x: the input frames tensor of shape (B, T, C, H, W), i.e. video with T frames\n    \"\"\"\n    def forward(self, x):\n        d = x.roll(-1, dims=1) - x\n        return d\n\nclass DifferentiableQuantization(nn.Module):\n    def __init__(self, num_bins=15, hardness=5.0, normalize_input=True, rescale_output=True):\n        \"\"\"\n        Args:\n            num_bins (int): Number of quantization bins N = 2^k (default 15).\n            hardness (float): Controls sigmoid sharpness; higher = closer to step function.\n            normalize_input (bool): Whether to normalize input to [0, num_bins] before quantizing.\n            rescale_output (bool): Whether to rescale output back to input's original value range.\n        \"\"\"\n        super().__init__()\n        self.num_bins = num_bins\n        self.hardness = hardness\n        self.normalize_input = normalize_input\n        self.rescale_output = rescale_output\n\n        # Initialize bin centers at [0.5, 1.5, ..., 14.5] for num_bins = 15\n        init_bins = torch.linspace(0.5, num_bins - 0.5, steps=num_bins)\n        self.bins = nn.Parameter(init_bins)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Input tensor of shape (B, T, C, H, W)\n        Returns:\n            Tensor: Quantized output of shape (B, T, C, H, W)\n        \"\"\"\n        orig_min, orig_max = x.min(), x.max() #TODO is it batch min/max?\n\n        if self.normalize_input:\n            qmin = 0.0\n            qmax = float(self.num_bins)\n            scale = (orig_max - orig_min) / (qmax - qmin)\n            scale = max(scale, 1e-4)\n            x = (x - orig_min) / (orig_max - orig_min + 1e-4) * (qmax - qmin)\n\n        # Expand for broadcasting\n        x_expanded = x.unsqueeze(-1)                        # Shape: [B, T, C, H, W, 1]\n        bin_centers = self.bins.view(1, 1, 1, 1, 1, -1).to(device)        # Shape: [1, 1, 1, 1, 1, num_bins]\n\n        # Sum of sigmoid activations\n        y = torch.sigmoid(self.hardness * (x_expanded - bin_centers)).sum(dim=-1)\n\n        if self.normalize_input and self.rescale_output:\n            y = y * scale + orig_min\n\n        return y\n","metadata":{"id":"pRgacPzE52Et","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4 BDQ encoder and label predictors","metadata":{"id":"IySrZFD-6Kx9"}},{"cell_type":"code","source":"class BDQEncoder(nn.Module):\n    \"\"\"\n    Sequentially combines the blur, difference and quantization parts\n    to form the BDQ encoder.\n    \"\"\"\n    def __init__(self, hardness=5.0):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            LearnableGaussian(),\n            Difference(),\n            DifferentiableQuantization(hardness=hardness),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: the input tensor (video frame).\n        \"\"\"\n        for layer in self.encoder:\n            x = layer.forward(x)\n\n        return x\n\n    # def freeze(self):\n    #     \"\"\"\n    #     Freezes the parameters to prevent/pause learning.\n    #     \"\"\"\n    #     for param in self.parameters():\n    #         param.requires_grad = False\n\n    # def unfreeze(self):\n    #     \"\"\"\n    #     Resumes learning for BDQ encoder parameters.\n    #     \"\"\"\n    #     for param in self.parameters():\n    #         param.requires_grad = True\n\nclass ActionRecognitionModel(nn.Module):\n    def __init__(self, fine_tune, num_classes = 400, id_to_classname = None):\n        super(ActionRecognitionModel, self).__init__()\n\n        # From action recognition file global variables\n        side_size = 256\n        mean = [0.45, 0.45, 0.45]\n        std = [0.225, 0.225, 0.225]\n        crop_size = 256\n        num_frames = 8\n        sampling_rate = 8\n        frames_per_second = 30\n        clip_duration = (num_frames * sampling_rate) / frames_per_second\n        self.start_sec = 0\n        self.end_sec = self.start_sec + clip_duration\n\n        model = torch.hub.load('facebookresearch/pytorchvideo', 'i3d_r50', pretrained=True)\n        model = model.eval()\n        model = model.to(device)\n        if fine_tune:\n            for param in model.parameters():\n                param.requires_grad = False\n            model.blocks[-1].proj = nn.Linear(in_features = model.blocks[-1].proj.in_features, out_features = num_classes)\n            for param in model.blocks[-1].proj.parameters():\n                param.requires_grad = True\n        self.model = model\n        self.transform = ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose([UniformTemporalSubsample(num_frames),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    ShortSideScale(size = side_size),\n                    CenterCrop((crop_size, crop_size))])\n        )\n        self.id_to_classname = id_to_classname\n\n    #accepts [B?, C=3, T=num_frames?, crop_size, crop_size]\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): input video (batched) of shape (B, T, C, H, W).\n\n        outputs:\n            y (string): predicted action label.\n        \"\"\"\n        # If not batched, make sample of 1 batch\n        if len(x.shape) == 4:\n            x = x.unsqueeze(dim=0)\n\n        # Transpose channel and temporal dimension\n        x = torch.transpose(x, -3, -4)\n        logits = self.model(x)  # Get prediction logits from 3d resnet. Shape: (B, num_classes)\n\n        # Apply softmax to get and return probabilities of each label\n        logits_softmax = F.softmax(logits, dim=1)\n\n        return logits_softmax\n\n    def test(self, video_path):\n        video = EncodedVideo.from_path(video_path)\n        video_data = video.get_clip(start_sec = self.start_sec, end_sec =self.end_sec)\n        video_data = self.transform(video_data)\n        inputs = video_data[\"video\"]\n        return self.predict(inputs)\n\n    def predict(self, inputs):\n        inputs = inputs.to(device)\n        preds = self.forward(inputs[None, ...])\n        post_act = torch.nn.Softmax(dim = 1)\n        preds = post_act(preds)\n        pred_classes = preds.topk(k = 1).indices[0]\n        pred_class_names = [self.id_to_classname[int(i)] for i in pred_classes]\n        return pred_class_names\n\n    # def freeze(self):\n    #     for param in self.parameters():\n    #         param.requires_grad = False\n\n    # def unfreeze(self):\n    #     for param in self.parameters():\n    #         param.requires_grad = True\n# Adapted from: https://pytorch.org/hub/facebookresearch_pytorchvideo_resnet/\n\nclass PrivacyAttributePredictor(nn.Module):\n    \"\"\"\n    Privacy Attribute Prediction Model.\n    Uses a 2D ResNet-50 to predict privacy attributes from BDQ-encoded video frames.\n    The softmax outputs from each frame are averaged.\n    \"\"\"\n    def __init__(self, num_privacy_classes, pretrained_resnet=True):\n        \"\"\"\n        Args:\n            num_privacy_classes (int): The number of privacy attribute classes to predict.\n            pretrained_resnet (bool): Whether to use ImageNet pre-trained weights for ResNet-50.\n        \"\"\"\n        super().__init__()\n        self.num_privacy_classes = num_privacy_classes\n\n        # Load a 2D ResNet-50 model\n        if pretrained_resnet:\n            self.resnet_feature_extractor = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        else:\n            self.resnet_feature_extractor = models.resnet50(weights=None)\n        for param in self.resnet_feature_extractor.parameters():\n            param.requires_grad = False\n        # Replace the final fully connected layer for the new number of privacy classes\n        num_ftrs = self.resnet_feature_extractor.fc.in_features\n        self.resnet_feature_extractor.fc = nn.Linear(num_ftrs, num_privacy_classes)\n        for param in self.resnet_feature_extractor.fc.parameters():\n            param.requires_grad = True\n\n    def forward(self, bdq_encoded_video):\n        \"\"\"\n        Forward pass for the privacy attribute predictor.\n\n        Args:\n            bdq_encoded_video (torch.Tensor): The output from the BDQ encoder.\n                Shape: (B, T, C, H, W), where\n                B = batch size\n                T = number of time steps/frames\n                C = number of channels\n                H = height\n                W = width\n\n        Returns:\n            torch.Tensor: Averaged softmax probabilities for privacy attributes.\n                          Shape: (B, num_privacy_classes)\n        \"\"\"\n        B, T, C, H, W = bdq_encoded_video.shape\n\n        # ResNet50 expects input of shape (N, C, H, W).\n        # We need to process each of the T frames for each video in the batch.\n        # Reshape to (B*T, C, H, W) to pass all frames through ResNet in one go.\n        video_reshaped_for_resnet = bdq_encoded_video.contiguous().view(B * T, C, H, W)\n\n        # Get logits from the ResNet feature extractor for all (B*T) frames\n        logits_all_frames = self.resnet_feature_extractor(video_reshaped_for_resnet) # Shape: (B*T, num_privacy_classes)\n\n        # Apply softmax to get probabilities for each frame\n        softmax_all_frames = F.softmax(logits_all_frames, dim=1) # Shape: (B*T, num_privacy_classes)\n\n        # Reshape back to (B, T, num_privacy_classes) to separate frames per video\n        softmax_per_frame_per_video = softmax_all_frames.view(B, T, self.num_privacy_classes)\n\n        # Average the softmax outputs over the T frames for each video in the batch\n        # as described in the paper (Section 4.2 Validation & Section 4.3 Results explanation).\n        averaged_softmax_predictions = torch.mean(softmax_per_frame_per_video, dim=1) # Shape: (B, num_privacy_classes)\n\n        return averaged_softmax_predictions\n\n    # def freeze(self):\n    #     for param in self.parameters():\n    #         param.requires_grad = False\n\n    # def unfreeze(self):\n    #     for param in self.parameters():\n    #         param.requires_grad = True\n","metadata":{"id":"_Rm_09zs6EK9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Define model training function","metadata":{"id":"v7uzedXx7CDW"}},{"cell_type":"code","source":"# Setup checkpointing\nCOLAB_PATH = os.getenv('COLAB_PATH')\nCHECKPOINT_PATH = \"checkpoints\" if COLAB_PATH is None else COLAB_PATH  # \"checkpoints/checkpoint_1.tar\"\nif not os.path.isdir(CHECKPOINT_PATH):\n    os.makedirs(CHECKPOINT_PATH)\n\n\ndef get_sorted_checkpoints():\n    checkpoints = []\n    try:\n        files = os.listdir(CHECKPOINT_PATH)\n    except FileNotFoundError:\n        return checkpoints\n    for file in files:\n        match = re.search(r'checkpoint_(\\d+)\\.tar$', file)\n        if match:\n            checkpoints.append((os.path.join(CHECKPOINT_PATH, file), int(match.group(1))))\n    checkpoints.sort(key=lambda x: x[1])\n    return checkpoints\n\ndef delete_old_checkpoints():\n    checkpoints = get_sorted_checkpoints()\n    if len(checkpoints) > 2:\n        for file, _ in checkpoints[:-2]:\n            os.remove(file)\n\ndef compute_accuracy(input, target_action, target_privacy):\n    \"\"\"\n    Computes action and privacy prediction accuracy\n    Args:\n        input: the input (batched) video tensor\n        target_action: target labels for action attribute\n        target_privacy: target labels for privacy attribute\n    \"\"\"\n    with torch.no_grad():\n        input_encoded = E.forward(input)\n        T_pred = T.forward(input_encoded).argmax(dim=1)\n        P_pred = P.forward(input_encoded).argmax(dim=1)\n\n        action_acc = torch.sum(T_pred == target_action)\n        privacy_acc = torch.sum(P_pred == target_privacy)\n\n        return action_acc, privacy_acc\n\ndef train_once(train_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n               action_loss: ActionLoss, privacy_loss: PrivacyLoss, optimizer_ET: Optimizer, optimizer_P: Optimizer):\n    \"\"\"\n    Function to perform one training epoch of adverserial training from https://arxiv.org/abs/2208.02459\n    Args:\n        train_dataloader: DataLoader for the training split of the KTH dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n        optimizer_ET: SGD optimizer for the encoder and action attribute predictor\n        optimizer_P: SGD optimizer for the privacy attribute predictor\n    \"\"\"\n    # Set all components to training mode\n    E.train()\n    T.train()\n    P.train()\n\n    total_loss_action = torch.tensor(0.)\n    total_loss_privacy = torch.tensor(0.)\n    total_acc_action = torch.tensor(0.)\n    total_acc_privacy = torch.tensor(0.)\n\n    for input, target_action, target_privacy in tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training epoch...\", unit=\"batch\", position=1, leave=False):\n        input = input.to(device)\n        target_action = target_action.to(device)\n        target_privacy = target_privacy.to(device)\n\n        # Reset gradients\n        # optimizer_P.zero_grad()\n        optimizer_ET.zero_grad()\n\n        # Freeze P, train E and T together\n        # P.freeze()\n        input_encoded = E.forward(input)\n        action_pred = T.forward(input_encoded)\n        frozen_privacy_pred = P.forward(input_encoded)\n        loss_action = action_loss.forward(action_pred, frozen_privacy_pred, target_action)\n        loss_action.backward()\n        optimizer_ET.step()\n\n        optimizer_P.zero_grad()\n\n        # Freeze E and T, unfreeze and train P\n        # P.unfreeze()\n        # E.freeze()\n        # T.freeze()\n        frozen_input_encoded = E.forward(input)\n        privacy_pred = P.forward(frozen_input_encoded)\n        loss_privacy = privacy_loss.forward(privacy_pred, target_privacy)\n        loss_privacy.backward()\n        optimizer_P.step()\n\n        # Unfreeze all models, record losses\n        # E.unfreeze()\n        # T.unfreeze()\n\n        # Compute statistics\n        acc_action, acc_privacy = compute_accuracy(input, target_action, target_privacy)\n\n        total_loss_action += loss_action.item()\n        total_loss_privacy += loss_privacy.item()\n\n        total_acc_action += acc_action.item()\n        total_acc_privacy += acc_privacy.item()\n\n    # Average out accuracies\n    total_acc_action /= len(train_dataloader.dataset)\n    total_acc_privacy /= len(train_dataloader.dataset)\n\n    return total_loss_action, total_loss_privacy, total_acc_action, total_acc_privacy\n\ndef validate_once(val_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n                  action_loss: ActionLoss, privacy_loss: PrivacyLoss):\n    \"\"\"\n    Function to perform one validation epoch of adverserial training from https://arxiv.org/abs/2208.02459\n    Args:\n        val_dataloader: DataLoader for the validation split of the KTH dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n    \"\"\"\n    E.eval()\n    T.eval()\n    P.eval()\n\n    with torch.no_grad():\n\n        total_loss_action = torch.tensor(0.)\n        total_loss_privacy = torch.tensor(0.)\n        total_acc_action = torch.tensor(0.)\n        total_acc_privacy = torch.tensor(0.)\n\n        for input, target_action, target_privacy in tqdm(val_dataloader, total=len(val_dataloader), desc=\"Validating epoch...\", unit=\"batch\", position=1, leave=False):\n            input = input.to(device)\n            target_action = target_action.to(device)\n            target_privacy = target_privacy.to(device)\n\n            # Perform evaluation with models on respective inputs\n            input_encoded = E.forward(input)\n            action_pred = T.forward(input_encoded)\n            privacy_pred = P.forward(input_encoded)\n\n            # Compute statistics\n            loss_action = action_loss.forward(action_pred, privacy_pred, target_action)\n            loss_privacy = privacy_loss.forward(privacy_pred, target_privacy)\n\n            acc_action, acc_privacy = compute_accuracy(input, target_action, target_privacy)\n\n            total_loss_action += loss_action.item()\n            total_loss_privacy += loss_privacy.item()\n            total_acc_action += acc_action.item()\n            total_acc_privacy += acc_privacy.item()\n\n        # Average out accuracies\n        total_acc_action /= len(val_dataloader.dataset)\n        total_acc_privacy /= len(val_dataloader.dataset)\n\n        return total_loss_action, total_loss_privacy, total_acc_action, total_acc_privacy\n\ndef adverserial_training(train_dataloader: DataLoader, val_dataloader: DataLoader, E: BDQEncoder, T: ActionRecognitionModel,\n                         P: PrivacyAttributePredictor, optimizer_ET: Optimizer, optimizer_P: Optimizer, scheduler_ET: LRScheduler,\n                         scheduler_P: LRScheduler, action_loss: ActionLoss, privacy_loss: PrivacyLoss, last_epoch=0, num_epochs=50):\n    \"\"\"\n    Function encapsulating the whole adverserial training process from https://arxiv.org/abs/2208.02459\n    Args:\n        train_dataloader: DataLoader for the training split of the KTH dataset\n        val_dataloader: DataLoader for the validation split of the KTH dataset\n        E: the BDQ encoder\n        T: 3d resnet50 for predicting target action attributes\n        P: 2d resnet50 for predicting target privacy attributes\n        optimizer_ET: SGD optimizer for the encoder and action attribute predictor\n        optimizer_P: SGD optimizer for the privacy attribute predictor\n        scheduler_ET: learning rate scheduler for updating learning rate each epoch for optimizer_ET\n        scheduler_P: learning rate scheduler for updating learning rate each epoch for optimizer_P\n        action_loss: criterion for optimizing action attribute prediction\n        privacy_loss: criterion for optimizing privacy attribute prediction\n        last_epoch (optional, int): checkpoint of last saved epoch\n        num_epochs (optional, int): number of epochs to train for (default=50)\n    \"\"\"\n    def save_checkpoint(epoch: int):\n        torch.save({\n            'E_state_dict': E.state_dict(),\n            'T_state_dict': T.state_dict(),\n            'P_state_dict': P.state_dict(),\n            'optim_ET_state_dict': optimizer_ET.state_dict(),\n            'optim_P_state_dict': optimizer_P.state_dict(),\n            'scheduler_ET_state_dict': scheduler_ET.state_dict(),\n            'scheduler_P_state_dict': scheduler_P.state_dict(),\n        }, os.path.join(CHECKPOINT_PATH, f\"checkpoint_{epoch}.tar\"))\n        delete_old_checkpoints()\n\n    action_accuracies = []\n    privacy_accuracies = []\n\n    with tqdm(range(last_epoch, num_epochs), total=num_epochs, initial=last_epoch, desc=\"Averserial training\", unit=\"epoch\", position=0, leave=True) as progress_loader:\n        for epoch in progress_loader:\n            train_loss_action, train_loss_privacy, train_acc_action, train_acc_privacy = train_once(train_dataloader=train_dataloader, E=E, T=T, P=P,\n                                                                                                    action_loss=action_loss, privacy_loss=privacy_loss,\n                                                                                                    optimizer_ET=optimizer_ET, optimizer_P=optimizer_P)\n\n            val_loss_action, val_loss_privacy, val_acc_action, val_acc_privacy = validate_once(val_dataloader=val_dataloader, E=E, T=T, P=P,\n                                                                                            action_loss=action_loss, privacy_loss=privacy_loss)\n\n            # Update learning rates\n            scheduler_ET.step()\n            scheduler_P.step()\n            save_checkpoint(epoch + 1)\n\n            # Display statistics\n            progress_loader.set_postfix(action_loss=val_loss_action.numpy(), privacy_loss=val_loss_privacy.numpy(),\n                                         action_accuracy=val_acc_action.numpy(), privacy_accuracy= val_acc_privacy.numpy())\n            action_accuracies.append(val_acc_action.item())\n            privacy_accuracies.append(val_acc_privacy.item())\n            progress_loader.refresh()\n            # print(f\"Epoch {epoch+1}/{num_epochs}, Action Loss: {val_loss_action:.4f}, Privacy Loss: {val_loss_privacy:.4f}\")\n            # print(f\"Action accuracy: {val_acc_action:.4f}, Privacy accuracy: {val_acc_privacy:.4f}\")\n    \n    # Free resources before plotting/logging\n    del train_dataloader\n    del val_dataloader\n    import gc\n    gc.collect()\n\n    return action_accuracies, privacy_accuracies\n\ndef load_train_checkpoint(E: BDQEncoder, T: ActionRecognitionModel, P: PrivacyAttributePredictor,\n               optim_ET: Optimizer, optim_P: Optimizer, scheduler_ET: LRScheduler, scheduler_P: LRScheduler, PATH: str | None):\n    if PATH is None:\n        return\n    checkpoint = torch.load(PATH, weights_only=True)\n    E.load_state_dict(checkpoint['E_state_dict'])\n    E.to(device)\n    T.load_state_dict(checkpoint['T_state_dict'])\n    T.to(device)\n    P.load_state_dict(checkpoint['P_state_dict'])\n    P.to(device)\n    optim_ET.load_state_dict(checkpoint['optim_ET_state_dict'])\n    optim_P.load_state_dict(checkpoint['optim_P_state_dict'])\n    scheduler_ET.load_state_dict(checkpoint['scheduler_ET_state_dict'])\n    scheduler_P.load_state_dict(checkpoint['scheduler_P_state_dict'])\n","metadata":{"id":"E2i3_uZb7IdW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Train the models","metadata":{"id":"ws48rAiM7OVn"}},{"cell_type":"markdown","source":"This might help with memory fragmentation","metadata":{"id":"JMe-bdS59fyF"}},{"cell_type":"code","source":"# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"id":"SAQoOLKiO3vI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{"id":"KzgQK8fnPgu0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set parameters according to https://arxiv.org/abs/2208.02459\nnum_epochs = 50\nlr = 0.001\nbatch_size = 4\nconsecutive_frames = 32\ncrop_size = (224, 224)\n\n# Load KTH dataset. Apply transformation sequence according to Section 4.2 in https://arxiv.org/abs/2208.02459\ntrain_transform = Compose([\n    ConsecutiveTemporalSubsample(consecutive_frames), # first, sample 32 consecutive frames\n    MultiScaleCrop(), # then, apply randomized multi-scale crop\n    Resize(crop_size), # then, resize to (224, 224)\n    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n])\ntrain_data = KTHBDQDataset(\n    root_dir=KTH_DATA_DIR,\n    json_path=KTH_LABELS_DIR,\n    transform=train_transform,\n    split=\"train\",\n)\ntrain_dataloader = DataLoader(\n    train_data,\n    batch_size=batch_size,\n    num_workers=4,\n)\n# Load validation dataset according to the same Section 4.2\nval_transform = Compose([\n    ConsecutiveTemporalSubsample(consecutive_frames), # first sample 32 consecutive frames\n    CenterCrop(crop_size),  # then, we apply a center crop of (224, 224) without scaling (resizing)\n    NormalizePixelValues(), # (also normalize pixel values for pytorch)\n])\nval_data = KTHBDQDataset(\n    root_dir=KTH_DATA_DIR,\n    json_path=KTH_LABELS_DIR,\n    transform=val_transform,\n    split=\"val\",\n)\nval_dataloader = DataLoader(\n    val_data,\n    batch_size=batch_size,\n    num_workers=4,\n)\n\n# Initialize the BDQEncoder (E), the action attribute predictor (T),\n# and the privacy attribute predictor (P)\nE = BDQEncoder(hardness=5.0).to(device)\nT = ActionRecognitionModel(fine_tune=True, num_classes=6).to(device)\nP = PrivacyAttributePredictor(num_privacy_classes=25).to(device)\n\n# Initialize optimizer, scheduler and loss functions\noptim_ET = SGD(params=list(E.parameters())+list(T.parameters()), lr=lr)\noptim_P = SGD(params=P.parameters(), lr=lr)\nscheduler_ET = CosineAnnealingLR(optimizer=optim_ET, T_max=num_epochs)\nscheduler_P = CosineAnnealingLR(optimizer=optim_P, T_max=num_epochs)\ncheckpoints = get_sorted_checkpoints()\nlast_checkpoint_path = None\nlast_epoch = 0\nif len(checkpoints) > 0:\n    last_checkpoint_path, last_epoch = checkpoints[-1]\nload_train_checkpoint(E, T, P, optim_ET, optim_P, scheduler_ET, scheduler_P, last_checkpoint_path)\ncriterion_action = ActionLoss(alpha=1)\ncriterion_privacy = PrivacyLoss()\n\naction_accuracies, privacy_accuracies = adverserial_training(train_dataloader=train_dataloader, val_dataloader=val_dataloader, E=E, T=T, P=P,\n                      optimizer_ET=optim_ET, optimizer_P=optim_P, scheduler_ET=scheduler_ET,\n                      scheduler_P=scheduler_P, action_loss=criterion_action, privacy_loss=criterion_privacy,\n                      last_epoch=last_epoch, num_epochs=num_epochs)","metadata":{"id":"by6pxvQC7OKX","outputId":"28714b42-bd19-40bb-8bd2-4e7736b3b9f8","trusted":true,"colab":{"referenced_widgets":["56beea86ac6f4b9ca3834e3c3fb46369","","e744a8f7d2de4643b6ba051647b4a064"]}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Logging and Visualization ","metadata":{}},{"cell_type":"code","source":"def save_quantizer_mapping(dq_module, output_csv_path=\"quant_steps.csv\", device=\"cpu\"):\n    # Parameters\n    num_bins = dq_module.num_bins\n    hardness = dq_module.hardness\n    normalize = dq_module.normalize_input\n    rescale = dq_module.rescale_output\n\n    # Input values in normalized space: [0, num_bins]\n    x_vals = torch.linspace(0, num_bins, 1000, device=device).view(-1, 1)\n\n    # Initial bin centers [0.5, 1.5, ..., 14.5]\n    init_bins = torch.linspace(0.5, num_bins - 0.5, steps=num_bins).to(device).view(1, -1)\n\n    # Learned bins\n    learned_bins = dq_module.bins.detach().to(device).view(1, -1)\n\n    # Quantization function: sum of sigmoids\n    def quant_output(x, bins):\n        return torch.sigmoid(hardness * (x - bins)).sum(dim=-1)\n\n    # Evaluate\n    with torch.no_grad():\n        y_init = quant_output(x_vals, init_bins)\n        y_learned = quant_output(x_vals, learned_bins)\n\n        # Optional: rescale output like your quantizer does\n        if rescale:\n            y_init = y_init * (1.0) + 0.0  # No orig_min/max: we stay in normalized space\n            y_learned = y_learned * (1.0) + 0.0\n\n        # Convert to numpy\n        x_vals_np = x_vals.squeeze().cpu().numpy()\n        y_init_np = y_init.squeeze().cpu().numpy()\n        y_learned_np = y_learned.squeeze().cpu().numpy()\n\n    # Save as CSV\n    df = pd.DataFrame({\n        \"input\": x_vals_np,\n        \"init_output\": y_init_np,\n        \"learned_output\": y_learned_np\n    })\n    df.to_csv(output_csv_path, index=False)\n\nsave_quantizer_mapping(E.encoder[2], \"quant_steps_kth.csv\", device=\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = list(range(last_epoch + 1, num_epochs + 1))\nlog_df = pd.DataFrame({\n    'epoch': epochs,\n    'action_accuracy': action_accuracies,\n    'privacy_accuracy': privacy_accuracies\n})\n\nlog_df['action_accuracy'] *= 100\nlog_df['privacy_accuracy'] *= 100\n\nlog_df.to_csv('accuracy_log_kth.csv', index=False)\n\nplt.figure()\nplt.plot(log_df['epoch'], log_df['action_accuracy'], label='Action Accuracy')\nplt.plot(log_df['epoch'], log_df['privacy_accuracy'], label='Privacy Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy (%) vs. Epoch')\nplt.legend()\nplt.grid(True)\nplt.savefig('accuracy_plot_kth.png')\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}