{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1b07fb",
   "metadata": {},
   "source": [
    "## Preprocess KTH Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37aa29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f54229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHBDQDataset(Dataset):\n",
    "    def __init__(self, root_dir, json_path, clip_len=32, resize=(128, 128), split=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to KTH folder (contains 'boxing', 'handclapping', etc.)\n",
    "            json_path (str): JSON file with start/end frame info per clip\n",
    "            clip_len (int): Number of frames to sample per clip\n",
    "            resize (tuple): Output frame size\n",
    "            split (str): Optional filter for 'train' / 'val' / 'test'\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_len = clip_len\n",
    "        self.resize = resize\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(resize),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            all_clips = json.load(f)\n",
    "\n",
    "        self.data = all_clips if split is None else [clip for clip in all_clips if clip.get(\"split\") == split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_video_path(self, label, video_id):\n",
    "        \"\"\"Construct video path using the '_uncomp' suffix.\"\"\"\n",
    "        return os.path.join(self.root_dir, label, f\"{video_id}_uncomp.avi\")\n",
    "\n",
    "\n",
    "    def _load_clip(self, video_path, start, end):\n",
    "        \"\"\"Extract a clip of frames from the video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        start = max(1, start)\n",
    "        end = min(end, total_frames)\n",
    "\n",
    "        indices = torch.linspace(start, end, steps=self.clip_len).long().tolist()\n",
    "        frames = []\n",
    "\n",
    "        for i in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_idx = i + 1\n",
    "            if frame_idx in indices:\n",
    "                img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                img = self.transform(img)  # [C, H, W]\n",
    "                frames.append(img)\n",
    "                if len(frames) == self.clip_len:\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.clip_len:\n",
    "            raise ValueError(f\"Clip too short: {video_path} ({start}-{end})\")\n",
    "\n",
    "        return torch.stack(frames)  # [T, C, H, W]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        video_path = self._get_video_path(entry[\"label\"], entry[\"video_id\"])\n",
    "        clip = self._load_clip(video_path, entry[\"start_frame\"], entry[\"end_frame\"])\n",
    "        return clip  # ready for the Blur module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c8109",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79f7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KTHBDQDataset(\n",
    "    root_dir=\"KTH\",\n",
    "    json_path=\"kth_clips.json\",\n",
    "    clip_len=32,\n",
    "    resize=(128, 128)\n",
    ")\n",
    "\n",
    "clip = dataset[0]  # shape [T, C, H, W]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
